{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48007063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from src.modeler import Modeler\n",
    "from src.prompt_manager import PromptManager\n",
    "from src.utils import generate_in_batches\n",
    "from src.pipeline import run_experiment\n",
    "import gc  # Import the garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccdba18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: src/paths.py\n",
    "from pathlib import Path\n",
    "\n",
    "# This file lives in 'src/'.\n",
    "# Its parent is 'src/'.\n",
    "# Its parent's parent is the project root.\n",
    "# .resolve() makes it a clean, absolute path.\n",
    "PROJECT_ROOT = Path(\"..\")\n",
    "\n",
    "# Define all your *other* key paths relative to this anchor\n",
    "CONFIG_PATH = PROJECT_ROOT / \"src/configs/config.yaml\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "\n",
    "# You can even pre-build paths from your config logic\n",
    "COMPANY_DEALS_RAW = RAW_DATA_DIR / \"COMPANY_DEALS.parquet\"\n",
    "COMPANY_DEALS_RESULTS = RESULTS_DIR / \"company_deals\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a093d",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3e0f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00cde907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import interpreter_login\n",
    "# interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b1a55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.results import ResultsContainer\n",
    "\n",
    "# prompt_template = prompts['7785d73041']\n",
    "\n",
    "# dataset_dir = Path(\"../data/QA_McGill_ratings.csv\")\n",
    "# dataset_name = dataset_dir.stem\n",
    "# experiment_name = \"QA_McGill\"\n",
    "\n",
    "# output_dir = '../results'\n",
    "\n",
    "# model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "# safe_model_name = model_name.replace('/', '-')\n",
    "\n",
    "# session_file = Path(\n",
    "#     f\"{output_dir}/{experiment_name}/\"\n",
    "#     f\"dataset-{dataset_name}_\"\n",
    "#     f\"model-{safe_model_name}_\"\n",
    "#     f\"prompt-{prompt_template.id}_\"\n",
    "#     f\"prefix-none.pt\"\n",
    "# )\n",
    "\n",
    "# results = ResultsContainer().load(session_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce90e1",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abbca3c",
   "metadata": {},
   "source": [
    "# MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2b22994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "pm = PromptManager(folder=Path(\"../prompts/BARTER_DEALS\"))\n",
    "prompt_templates = pm.load_all()\n",
    "prompt_template = prompt_templates['f686fa3259']\n",
    "\n",
    "dataset_dir = Path('../data/raw/BARTER_DEALS.parquet')\n",
    "df = pd.read_parquet(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c5437ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa7b6bb91de440c8f33c772f6bbdf74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding side: left\n"
     ]
    }
   ],
   "source": [
    "variable_names = ['deal_text']\n",
    "prompts = prompt_template.render_many(df[variable_names][:1])\n",
    "\n",
    "model_name = 'google/gemma-3-4b-it'\n",
    "modeler = Modeler(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e2f865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerateDecoderOnlyOutput(sequences=tensor([[     2,      2,    105,   2364,    107,   3048,    795,    577,   2238,\n",
       "            496,   3772,   1816, 236761,    107,  11069,   4209,    563,    531,\n",
       "           3136,    506,   6782,   3325,    529,    506,   3772,   1816,    580,\n",
       "            496,   5559,    699, 236743, 236770,    531, 236743, 236810, 236761,\n",
       "            107,  16904,   3938,    506,  11995,  14470, 236761,    108, 134478,\n",
       "           1816, 236787,    107, 236775, 236792,   8714,  11109,  28007,    563,\n",
       "          38564,    496,   1944,   2299,    529,  44100,    531,   3517,    672,\n",
       "           7672, 236764,   3353, 236743, 236778, 236800, 236764,    573,    496,\n",
       "           6382,   3004,   8363,   4888,    506,  14871,    532,    528,    506,\n",
       "           3207,    529,  28007, 236743, 246272,    107,   3048, 236858,    859,\n",
       "            577,  28172,    684,    587,   8714, 236858, 236751,   9578,   2434,\n",
       "            532,   5908,    496,   9813,  62974,    528,  10764, 236761,    669,\n",
       "           5671, 236787,   2619,  10103,  58442,    506,  14871, 236764,    506,\n",
       "           1610, 236764,    532,    506,   6782,   2707, 236764,    618,   1388,\n",
       "            618,  11690,  19034,    529,  28007, 236761,    107,   2094,    563,\n",
       "            496,   6382, 121463,   5506,   2192,    611, 236858,    859,   5908,\n",
       "          11361, 236778, 236771, 236771,    573,    506,   2948,  35381,   5719,\n",
       "         236761,    108,   3689,    611,    974,    107, 246326,  11361, 236778,\n",
       "         236771, 236771,  15880,    107, 251251,   9617,   2802,    531,    587,\n",
       "           8714,  11109,    580,   3353, 236743, 236778, 236800,    568,   1708,\n",
       "            886,   1589, 236768,    107, 251332,  37799,    522,    900,    580,\n",
       "         236772,   8777,  14984,    699,    587,   8714, 236858, 236751,   9578,\n",
       "           2434, 236775,    106,    107,    105,   4368,    107, 236812]],\n",
       "       device='cuda:0'), scores=None, logits=(tensor([[-13.8125,   2.9531,   4.3125,  ...,  -0.0571,  -0.0708,  -0.0732]],\n",
       "       device='cuda:0'),), attentions=None, hidden_states=None, past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer, DynamicSlidingWindowLayer]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeler.set_token_constraints(list(\"12345\"))\n",
    "modeler.generate_chat(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31abe261",
   "metadata": {},
   "source": [
    "# Barter: Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56810e7b",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9504d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'mkdir'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m df = pd.read_parquet(dataset_dir)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Prompts\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m pm = \u001b[43mPromptManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../prompts/Barter_deals\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m prompts = pm.load_all()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# filter prompts that specifically constrain output to 5 unique tokens\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\AI_thesis\\src\\prompt_manager.py:151\u001b[39m, in \u001b[36mPromptManager.__init__\u001b[39m\u001b[34m(self, folder)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, folder: Path):\n\u001b[32m    150\u001b[39m     \u001b[38;5;28mself\u001b[39m.folder = folder\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# The manager now holds the state of all loaded prompts\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.prompt_templates: Dict[\u001b[38;5;28mstr\u001b[39m, PromptTemplate] = {}\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'mkdir'"
     ]
    }
   ],
   "source": [
    "# data and templates\n",
    "dataset_dir = Path('../data/raw/BARTER_DEALS.parquet')\n",
    "df = pd.read_parquet(dataset_dir)\n",
    "\n",
    "\n",
    "# Prompts\n",
    "pm = PromptManager(folder=Path(\"../prompts/Barter_deals\"))\n",
    "prompts = pm.load_all()\n",
    "# filter prompts that specifically constrain output to 5 unique tokens\n",
    "prompts = pm.get_filtered_prompts(required_tags=['tokens_5'])\n",
    "assistant_prefices = [\"\", \"Rating: \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f33053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['deal_text'] = df['description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables\n",
    "experiment_name = \"Barter_deals\"\n",
    "output_dir = '../results'\n",
    "dataset_name = dataset_dir.stem\n",
    "variable_names = ['deal_text']\n",
    "batch_size = 2\n",
    "id_col = 'deal_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbeec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Qwen/Qwen3-4B-Instruct-2507\", \"google/gemma-3-1b-it\",\n",
    "          'mistralai/Mistral-7B-Instruct-v0.2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc95bc2",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = Path('../results/MCGILL_QA_FEEDBACK')\n",
    "# for x in p.iterdir():\n",
    "#     x.rename(p/x.name.replace('QA_McGill_ratings', 'mcgill_qa_feedback'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b74dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def generate_output_filename(dataset_name, model_name, prompt_template_id, assistant_prefix):\n",
    "    safe_model_name = model_name.replace('/', '-')\n",
    "    if assistant_prefix == \"\":\n",
    "        safe_prefix = \"none\"\n",
    "    elif \"Rating\" in assistant_prefix:\n",
    "        safe_prefix = \"rating\"\n",
    "    else:\n",
    "        # A simple hash fallback for any other prefix\n",
    "        safe_prefix = f\"{hashlib.sha1(assistant_prefix.encode()).hexdigest()[:6]}\"\n",
    "\n",
    "    output_filename = (\n",
    "        f\"dataset-{dataset_name}_\"\n",
    "        f\"model-{safe_model_name}_\"\n",
    "        f\"prompt-{prompt_template_id}_\"\n",
    "        f\"prefix-{safe_prefix}.pt\"\n",
    "    )\n",
    "\n",
    "    return output_filename\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from src.prompt_manager import PromptManager\n",
    "from src.modeler import Modeler\n",
    "from src.pipeline import run_experiment\n",
    "import gc  # Import the garbage collector\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "with open('../src/configs/config.yaml', 'r') as f:\n",
    "    full_config = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "active_analysis_name = full_config['active_analysis']\n",
    "generation_config = full_config['generation'][active_analysis_name]\n",
    "batch_size = full_config['generation']['batch_size']\n",
    "\n",
    "PROMPT_TEMPLATES_DIR = Path(generation_config['paths']['prompt_templates_dir'])\n",
    "OUTPUT_DIR = Path(generation_config['paths']['output_dir'])\n",
    "MODELS = generation_config['models']\n",
    "RAW_DATA_PATH = Path(generation_config['paths']['raw_data_path'])\n",
    "BATCH_SIZE = full_config['generation']['batch_size']\n",
    "VARIABLE_NAMES = generation_config['variable_names']\n",
    "ID_COL = generation_config['id_col']\n",
    "TOP_K = generation_config['top_k']\n",
    "ASSISTANT_PREFICES = generation_config['assistant_prefices']\n",
    "\n",
    "# Prompts\n",
    "pm = PromptManager(folder=PROMPT_TEMPLATES_DIR)\n",
    "prompt_templates = pm.load_all()\n",
    "# filter prompts that specifically constrain output to 5 unique tokens\n",
    "prompt_templates = pm.get_filtered_prompts(required_tags=['tokens_5'])\n",
    "\n",
    "df = pd.read_parquet(RAW_DATA_PATH)\n",
    "torch.cuda.empty_cache()\n",
    "for model_name in MODELS:\n",
    "    modeler = Modeler(model_name)\n",
    "    # modeler.set_token_constraints(token_constraints)\n",
    "    for prefix in ASSISTANT_PREFICES:\n",
    "        pm.set_prefix_for_prompts(prefix)\n",
    "        for prompt_template in prompt_templates.values():\n",
    "            OUTPUT_PATH = OUTPUT_DIR / generate_output_filename(active_analysis_name, model_name, prompt_template.id, prefix)\n",
    "            # Configure custom logits processor for constrained output\n",
    "            token_constraints = prompt_template.constrained_output\n",
    "            modeler.set_token_constraints(token_constraints)\n",
    "\n",
    "            # Run experiment pipeline\n",
    "            run_experiment(df[:2000],\n",
    "                           modeler,\n",
    "                           prompt_template,\n",
    "                           OUTPUT_PATH,\n",
    "                           model_name,\n",
    "                           VARIABLE_NAMES,\n",
    "                           BATCH_SIZE,\n",
    "                           ID_COL,\n",
    "                           top_k=TOP_K,\n",
    "                           assistant_prefix=prefix)\n",
    "\n",
    "    print(f\"Releasing VRAM from model: {model_name}\")\n",
    "\n",
    "    # Explicitly delete the model and tokenizer objects\n",
    "    # This tells Python they can be garbage collected\n",
    "    del modeler.model\n",
    "    del modeler.tokenizer\n",
    "    del modeler\n",
    "\n",
    "    # Run Python's garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    # Tell PyTorch to empty its CachingAllocator\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d93ebd",
   "metadata": {},
   "source": [
    "------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0327f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16140db962b44421b334ebc4e166d298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding side: left\n",
      "Loaded 10 results from ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-Qwen-Qwen3-4B-Instruct-2507_prompt-8770599519_prefix-none.pt\n",
      "Processing 1990 new IDs...\n",
      "Processing 1990 prompts in 995 batches of size 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bb37a9dfb84b9ea2a417f024574f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Batches:   0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-Qwen-Qwen3-4B-Instruct-2507_prompt-8770599519_prefix-none.pt\n",
      "Loaded 10 results from ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-Qwen-Qwen3-4B-Instruct-2507_prompt-f686fa3259_prefix-none.pt\n",
      "Processing 1990 new IDs...\n",
      "Processing 1990 prompts in 995 batches of size 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808563e211674bfcbbf4c17e15abc4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Batches:   0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-Qwen-Qwen3-4B-Instruct-2507_prompt-f686fa3259_prefix-none.pt\n",
      "Loaded 10 results from ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-Qwen-Qwen3-4B-Instruct-2507_prompt-8770599519_prefix-rating.pt\n",
      "Processing 1990 new IDs...\n",
      "Processing 1990 prompts in 995 batches of size 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1165ac9d617b4d08937eaa93406f1047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Batches:   0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-Qwen-Qwen3-4B-Instruct-2507_prompt-8770599519_prefix-rating.pt\n",
      "Loaded 10 results from ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-Qwen-Qwen3-4B-Instruct-2507_prompt-f686fa3259_prefix-rating.pt\n",
      "Processing 1990 new IDs...\n",
      "Processing 1990 prompts in 995 batches of size 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bd23ac92e245f9b0d7a1fa88452fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Batches:   0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-Qwen-Qwen3-4B-Instruct-2507_prompt-f686fa3259_prefix-rating.pt\n",
      "Releasing VRAM from model: Qwen/Qwen3-4B-Instruct-2507\n",
      "Using device: cuda\n",
      "Padding side: left\n",
      "Loaded 10 results from ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-google-gemma-3-1b-it_prompt-8770599519_prefix-none.pt\n",
      "Processing 1990 new IDs...\n",
      "Processing 1990 prompts in 995 batches of size 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bf457b8870440da7d1e0f26932587f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Batches:   0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-google-gemma-3-1b-it_prompt-8770599519_prefix-none.pt\n",
      "Loaded 10 results from ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-google-gemma-3-1b-it_prompt-f686fa3259_prefix-none.pt\n",
      "Processing 1990 new IDs...\n",
      "Processing 1990 prompts in 995 batches of size 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5270e9543a9e41e98454a0e5d83065f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Batches:   0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-google-gemma-3-1b-it_prompt-f686fa3259_prefix-none.pt\n",
      "Loaded 10 results from ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-google-gemma-3-1b-it_prompt-8770599519_prefix-rating.pt\n",
      "Processing 1990 new IDs...\n",
      "Processing 1990 prompts in 995 batches of size 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c66df334f0f468cbc8f920031b98a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Batches:   0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-google-gemma-3-1b-it_prompt-8770599519_prefix-rating.pt\n",
      "Loaded 10 results from ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-google-gemma-3-1b-it_prompt-f686fa3259_prefix-rating.pt\n",
      "Processing 1990 new IDs...\n",
      "Processing 1990 prompts in 995 batches of size 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66aec2a47bbe4c4ca44b820aacafd790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Batches:   0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-google-gemma-3-1b-it_prompt-f686fa3259_prefix-rating.pt\n",
      "Releasing VRAM from model: google/gemma-3-1b-it\n",
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41865cfd0a34125a064340349bc10f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding side: left\n",
      "Loaded 10 results from ..\\results\\Barter_deals\\dataset-Barter_deals_dirty_model-mistralai-Mistral-7B-Instruct-v0.2_prompt-8770599519_prefix-none.pt\n",
      "Processing 1990 new IDs...\n",
      "Processing 1990 prompts in 995 batches of size 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cf7093378840e0b162551390f2e070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Batches:   0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m         modeler.set_token_constraints(token_constraints)\n\u001b[32m     12\u001b[39m         \u001b[38;5;66;03m# Run experiment pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mmodeler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                       \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m                       \u001b[49m\u001b[43massistant_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReleasing VRAM from model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Explicitly delete the model and tokenizer objects\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# This tells Python they can be garbage collected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\AI_thesis\\src\\pipeline.py:64\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(df, modeler, experiment_name, prompt_template, model_name, dataset_name, output_dir, variable_names, batch_size, id_col, top_k, assistant_prefix)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_new)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m new IDs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m prompts = prompt_template.render_many(df_new[variable_names])\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m sequences_list, logits_list = \u001b[43mgenerate_in_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodeler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodeler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# max_len = max(t.shape[1] for t in sequences_list)\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# pad_token_id = modeler.tokenizer.pad_token_id\u001b[39;00m\n\u001b[32m     73\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Concatenate the list of padded tensors into a single tensor\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# final_sequences_tensor = torch.cat(padded_sequences_list, dim=0)\u001b[39;00m\n\u001b[32m     82\u001b[39m final_logits_tensor = torch.cat(logits_list, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\AI_thesis\\src\\utils.py:37\u001b[39m, in \u001b[36mgenerate_in_batches\u001b[39m\u001b[34m(modeler, conversations, batch_size, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m batch_prompts = conversations[i:i + batch_size]\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Run generation and append the entire output object\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m model_output = \u001b[43mmodeler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m batch_sequences = torch.split(model_output.sequences, \u001b[32m1\u001b[39m, dim=\u001b[32m0\u001b[39m)\n\u001b[32m     40\u001b[39m sequences_list.extend(batch_sequences)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\AI_thesis\\src\\modeler.py:190\u001b[39m, in \u001b[36mModeler.generate_chat\u001b[39m\u001b[34m(self, prompts, max_new_tokens, **gen_kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(\n\u001b[32m    181\u001b[39m     formatted_prompts,\n\u001b[32m    182\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m     max_length=\u001b[38;5;28mself\u001b[39m.model_max_len\n\u001b[32m    187\u001b[39m ).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# 3. Generate the output (this part remains the same)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m model_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwargs\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\transformers\\generation\\utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:433\u001b[39m, in \u001b[36mMistralForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    414\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    415\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    416\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    418\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    431\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:369\u001b[39m, in \u001b[36mMistralModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    366\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    381\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    382\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    383\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:231\u001b[39m, in \u001b[36mMistralDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    230\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:182\u001b[39m, in \u001b[36mMistralAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m attn_output, attn_weights = attention_interface(\n\u001b[32m    170\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    171\u001b[39m     query_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m     **kwargs,\n\u001b[32m    179\u001b[39m )\n\u001b[32m    181\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:532\u001b[39m, in \u001b[36mLinear4bit.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    529\u001b[39m bias = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias.to(\u001b[38;5;28mself\u001b[39m.compute_dtype)\n\u001b[32m    530\u001b[39m weight = \u001b[38;5;28mself\u001b[39m.weight.t()\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m.to(inp_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:448\u001b[39m, in \u001b[36mmatmul_4bit\u001b[39m\u001b[34m(A, B, quant_state, out, bias)\u001b[39m\n\u001b[32m    446\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\autograd\\function.py:581\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    579\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    580\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    586\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    587\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    588\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    589\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:373\u001b[39m, in \u001b[36mMatMul4Bit.forward\u001b[39m\u001b[34m(ctx, A, B, out, bias, quant_state)\u001b[39m\n\u001b[32m    369\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.empty(A.shape[:-\u001b[32m1\u001b[39m] + B_shape[:\u001b[32m1\u001b[39m], dtype=A.dtype, device=A.device)\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m output = torch.nn.functional.linear(A, \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m.to(A.dtype).t(), bias)\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[32m    376\u001b[39m ctx.state = quant_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\bitsandbytes\\functional.py:984\u001b[39m, in \u001b[36mdequantize_4bit\u001b[39m\u001b[34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[39m\n\u001b[32m    981\u001b[39m     absmax = quant_state.absmax\n\u001b[32m    983\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m quant_state.nested:\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m     absmax = \u001b[43mdequantize_blockwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabsmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    985\u001b[39m     absmax += quant_state.offset\n\u001b[32m    986\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m absmax.dtype != torch.float32:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\bitsandbytes\\functional.py:709\u001b[39m, in \u001b[36mdequantize_blockwise\u001b[39m\u001b[34m(A, quant_state, absmax, code, out, blocksize, nested)\u001b[39m\n\u001b[32m    699\u001b[39m     torch.ops.bitsandbytes.dequantize_blockwise.out(\n\u001b[32m    700\u001b[39m         A,\n\u001b[32m    701\u001b[39m         absmax,\n\u001b[32m   (...)\u001b[39m\u001b[32m    705\u001b[39m         out=out,\n\u001b[32m    706\u001b[39m     )\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbitsandbytes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdequantize_blockwise\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mabsmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\_ops.py:841\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\_compile.py:53\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     51\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:1044\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1042\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1046\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\torch\\library.py:732\u001b[39m, in \u001b[36m_impl.<locals>.register_.<locals>.func_no_dynamo\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;129m@torch\u001b[39m._disable_dynamo\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc_no_dynamo\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\bitsandbytes\\backends\\cuda\\ops.py:251\u001b[39m, in \u001b[36m_\u001b[39m\u001b[34m(A, absmax, code, blocksize, dtype)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;129m@register_kernel\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbitsandbytes::dequantize_blockwise\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_\u001b[39m(A: torch.Tensor, absmax: torch.Tensor, code: torch.Tensor, blocksize: \u001b[38;5;28mint\u001b[39m, dtype: torch.dtype) -> torch.Tensor:\n\u001b[32m    250\u001b[39m     out = torch.empty_like(A, dtype=dtype)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     \u001b[43m_dequantize_blockwise_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabsmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wouter Barter\\Documents\\AI_thesis\\venv312\\Lib\\site-packages\\bitsandbytes\\backends\\cuda\\ops.py:299\u001b[39m, in \u001b[36m_dequantize_blockwise_impl\u001b[39m\u001b[34m(A, absmax, code, blocksize, dtype, out)\u001b[39m\n\u001b[32m    297\u001b[39m     lib.cdequantize_blockwise_bf16(*args)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dtype == torch.float32:\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcdequantize_blockwise_fp32\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    modeler = Modeler(model_name)\n",
    "    # modeler.set_token_constraints(token_constraints)\n",
    "    for prefix in assistant_prefices:\n",
    "        pm.set_prefix_for_prompts(prefix)\n",
    "\n",
    "        for prompt_template in prompts.values():\n",
    "            # Configure custom logits processor for constrained output\n",
    "            token_constraints = prompt_template.constrained_output\n",
    "            modeler.set_token_constraints(token_constraints)\n",
    "\n",
    "            # Run experiment pipeline\n",
    "            run_experiment(df[:2000],\n",
    "                           modeler,\n",
    "                           experiment_name,\n",
    "                           prompt_template,\n",
    "                           model_name,\n",
    "                           dataset_name,\n",
    "                           output_dir,\n",
    "                           variable_names,\n",
    "                           batch_size,\n",
    "                           id_col,\n",
    "                           top_k=1000,\n",
    "                           assistant_prefix=prefix)\n",
    "\n",
    "    print(f\"Releasing VRAM from model: {model_name}\")\n",
    "\n",
    "    # Explicitly delete the model and tokenizer objects\n",
    "    # This tells Python they can be garbage collected\n",
    "    del modeler.model\n",
    "    del modeler.tokenizer\n",
    "    del modeler\n",
    "\n",
    "    # Run Python's garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    # Tell PyTorch to empty its CachingAllocator\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55edf7",
   "metadata": {},
   "source": [
    "#TODO: RuntimeError: Parent directory ..\\results\\Barter_deals does not exist. from results.save in ResultsContainer\n",
    "\n",
    "Also, have not generated the data for Mistral instruct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6793a",
   "metadata": {},
   "source": [
    "# McGill: Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6b492",
   "metadata": {},
   "source": [
    "The dataset that I use as sample data is the \"feedbackQA\" dataset (from McGill university). The dataset contains pairs of questions and answers. Human evaluators were asked to evaluate the answers (the \"gold labels\"). It serves as a good sample dataset for developing my LLM evaluator: the methods that I found in the literature can be applied to any domain (not just advertising) so they can be tested on this high-quality dataset, and of course the dataset is interchangeable with the dataset from Barter (internship company). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56b601b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('../data/raw/MCGILL_QA_FEEDBACK.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7603b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "056584d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Unnamed: 0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "passage",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "feedback",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "domain",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "explanation_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "explanation_2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "input_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "60f5aab1-a6bc-4a75-bc47-c298e9c1d7c8",
       "rows": [
        [
         "0",
         "0",
         "How do I get help finding a job?",
         "{'passage_id': 140, 'reference': {'page_title': 'Coronavirus (COVID-19) information for job seekers', 'section_content': 'If you are a current job seeker or participant, this fact sheet provides\\nimportant information about mutual obligation requirements, appointments with\\nyour provider, and what to do if you are self-isolating:\\n\\nInformation for job seekers and participants\\n\\nIf you are participating in the ParentsNext program, this fact sheet provides\\nimportant information about your activities and appointments.\\n\\n\\nInformation for ParentsNext participants\\n\\n\\nParentsNext participants Frequently Asked Questions\\n\\n\\nIf you are a New Business Assistance with NEIS participant, these Frequently\\nAsked Questions (FAQ) provides information about accessing the Coronavirus\\nSupplement and what support is available during this time:\\n\\nNew Business Assistance with NEIS participants - Frequently Asked Questions\\n\\nIf you are a New Business Assistance with NEIS provider, these Frequently\\nAsked Questions (FAQ) provides information about supporting NEIS participants\\nduring the Coronavirus situation.\\n\\nNew Business Assistance with NEIS providers  Frequently Asked Questions\\n\\n*[NEIS]: New Enterprise Incentive Scheme', 'section_content_html': '<p>If you are a current job seeker or participant, this fact sheet provides\\nimportant information about mutual obligation requirements, appointments with\\nyour provider, and what to do if you are self-isolating:</p>\\n<ul>\\n<li><a href=\"https://docs.employment.gov.au/node/47535\">Information for job seekers and participants</a></li>\\n</ul>\\n<p>If you are participating in the ParentsNext program, this fact sheet provides\\nimportant information about your activities and appointments.</p>\\n<ul>\\n<li>\\n<p><a href=\"https://docs.employment.gov.au/node/47551\">Information for ParentsNext participants</a></p>\\n</li>\\n<li>\\n<p><a href=\"https://docs.employment.gov.au/node/47550\">ParentsNext participants Frequently Asked Questions</a></p>\\n</li>\\n</ul>\\n<p>If you are a New Business Assistance with NEIS participant, these Frequently\\nAsked Questions (FAQ) provides information about accessing the Coronavirus\\nSupplement and what support is available during this time:</p>\\n<ul>\\n<li><a href=\"https://docs.employment.gov.au/node/47552\">New Business Assistance with NEIS participants - Frequently Asked Questions</a></li>\\n</ul>\\n<p>If you are a New Business Assistance with NEIS provider, these Frequently\\nAsked Questions (FAQ) provides information about supporting NEIS participants\\nduring the Coronavirus situation.</p>\\n<ul>\\n<li><a href=\"http://\\u200bdocs.employment.gov.au/node/47592\">New Business Assistance with NEIS providers  Frequently Asked Questions</a></li>\\n</ul>\\n<p>*[NEIS]: New Enterprise Incentive Scheme</p>', 'section_headers': ['Exisiting job seekers'], 'selected_span': None, 'selection_span': None}, 'reference_type': 'Passage_only', 'source': 'Australia', 'uri': 'https://www.dese.gov.au/covid-19/job-seekers'}",
         "['Has a link to detailed information about government programs for job seekers.', 'This answer provides a link for job searches, which is good, but also provides links to unrelated sites.']",
         "['Excellent', 'Could be Improved']",
         "Australia",
         "Excellent",
         "Has a link to detailed information about government programs for job seekers.",
         "Could be Improved",
         "This answer provides a link for job searches, which is good, but also provides links to unrelated sites.",
         "4",
         "2",
         "If you are a current job seeker or participant, this fact sheet provides\nimportant information about mutual obligation requirements, appointments with\nyour provider, and what to do if you are self-isolating:\n\nInformation for job seekers and participants\n\nIf you are participating in the ParentsNext program, this fact sheet provides\nimportant information about your activities and appointments.\n\n\nInformation for ParentsNext participants\n\n\nParentsNext participants Frequently Asked Questions\n\n\nIf you are a New Business Assistance with NEIS participant, these Frequently\nAsked Questions (FAQ) provides information about accessing the Coronavirus\nSupplement and what support is available during this time:\n\nNew Business Assistance with NEIS participants - Frequently Asked Questions\n\nIf you are a New Business Assistance with NEIS provider, these Frequently\nAsked Questions (FAQ) provides information about supporting NEIS participants\nduring the Coronavirus situation.\n\nNew Business Assistance with NEIS providers  Frequently Asked Questions\n\n*[NEIS]: New Enterprise Incentive Scheme",
         "eff9e000675931f5"
        ],
        [
         "1",
         "1",
         "How do I get help finding a job?",
         "{'passage_id': 139, 'reference': {'page_title': 'Coronavirus (COVID-19) information for job seekers', 'section_content': 'In this rapidly changing jobs market the Australian Government is supporting\\nbusinesses and those Australians looking for work.\\nWhile many businesses have been adversely affected by COVID-19 and are\\nreducing their workforces, there are some areas of the economy which have an\\nincreased demand for workers.\\nThis includes jobs in health and care sectors, transport and logistics, some\\nareas of retail, mining and mining services, manufacturing, agriculture and\\ngovernment sectors, among others.\\nThe Jobs Hub helps you find advertised vacancies.', 'section_content_html': '<p>In this rapidly changing jobs market the Australian Government is supporting\\nbusinesses and those Australians looking for work.</p>\\n<p>While many businesses have been adversely affected by COVID-19 and are\\nreducing their workforces, there are some areas of the economy which have an\\nincreased demand for workers.</p>\\n<p>This includes jobs in health and care sectors, transport and logistics, some\\nareas of retail, mining and mining services, manufacturing, agriculture and\\ngovernment sectors, among others.</p>\\n<p>The <a href=\"/node/94\">Jobs Hub helps you find advertised vacancies</a>.</p>', 'section_headers': ['Jobs Hub'], 'selected_span': None, 'selection_span': None}, 'reference_type': 'Passage_only', 'source': 'Australia', 'uri': 'https://www.dese.gov.au/covid-19/job-seekers'}",
         "['A link to a job search website is included, as well as a list of job sectors that are hiring at this time.', 'Includes a link to a Jobs Hub page, which is be helpful for job seekers.']",
         "['Excellent', 'Excellent']",
         "Australia",
         "Excellent",
         "A link to a job search website is included, as well as a list of job sectors that are hiring at this time.",
         "Excellent",
         "Includes a link to a Jobs Hub page, which is be helpful for job seekers.",
         "4",
         "4",
         "In this rapidly changing jobs market the Australian Government is supporting\nbusinesses and those Australians looking for work.\nWhile many businesses have been adversely affected by COVID-19 and are\nreducing their workforces, there are some areas of the economy which have an\nincreased demand for workers.\nThis includes jobs in health and care sectors, transport and logistics, some\nareas of retail, mining and mining services, manufacturing, agriculture and\ngovernment sectors, among others.\nThe Jobs Hub helps you find advertised vacancies.",
         "c8be913323f10444"
        ],
        [
         "2",
         "2",
         "How do I get help finding a job?",
         "{'passage_id': 126, 'reference': {'page_title': 'Coronavirus (COVID-19) information and support', 'section_content': 'To further assist job seekers to prepare for and move quickly back into work,\\nthe Morrison Government is also bringing forward the Employment Fund credit\\nfor the most job-ready job seekers to be available immediately, rather than\\nafter 13 weeks, which is currently the case.\\nThis change will apply to job seekers who commence in jobactive from 4 May\\n2020.\\nThe Employment Fund can be used by employment service providers to purchase\\nwork-related items including licenses, equipment, training or qualifications\\nto ensure workers are prepared and able to take up critical roles in the\\neconomy.', 'section_content_html': '<p>To further assist job seekers to prepare for and move quickly back into work,\\nthe Morrison Government is also bringing forward the Employment Fund credit\\nfor the most job-ready job seekers to be available immediately, rather than\\nafter 13 weeks, which is currently the case.</p>\\n<p>This change will apply to job seekers who commence in jobactive from 4 May\\n2020.</p>\\n<p>The Employment Fund can be used by employment service providers to purchase\\nwork-related items including licenses, equipment, training or qualifications\\nto ensure workers are prepared and able to take up critical roles in the\\neconomy.</p>', 'section_headers': ['Employment Fund'], 'selected_span': None, 'selection_span': None}, 'reference_type': 'Passage_only', 'source': 'Australia', 'uri': 'https://www.dss.gov.au/about-the-department/coronavirus-covid-19-information-and-support'}",
         "['Talks about tax credits for businesses that hire new workers. Not helpful for someone looking for a job.', 'This answer discusses the Employment Fund, which could help people to become more employable. It does not offer specific information on jobs that are in demand or how to find one though.']",
         "['Bad', 'Acceptable']",
         "Australia",
         "Bad",
         "Talks about tax credits for businesses that hire new workers. Not helpful for someone looking for a job.",
         "Acceptable",
         "This answer discusses the Employment Fund, which could help people to become more employable. It does not offer specific information on jobs that are in demand or how to find one though.",
         "1",
         "3",
         "To further assist job seekers to prepare for and move quickly back into work,\nthe Morrison Government is also bringing forward the Employment Fund credit\nfor the most job-ready job seekers to be available immediately, rather than\nafter 13 weeks, which is currently the case.\nThis change will apply to job seekers who commence in jobactive from 4 May\n2020.\nThe Employment Fund can be used by employment service providers to purchase\nwork-related items including licenses, equipment, training or qualifications\nto ensure workers are prepared and able to take up critical roles in the\neconomy.",
         "31effc925bc04105"
        ],
        [
         "3",
         "3",
         "If I am in Australia on a worker holiday marker visa, can I apply for a second visa because I work in a critical COVID-19 area?",
         "{'passage_id': 581, 'reference': {'page_title': 'Frequently Asked Questions', 'section_content': \"No. Existing arrangements for specified work will remain in place but the\\nAustralian Government has announced temporary measures to assist WHMs working\\nin critical sectors who are not eligible to apply for a further visa.\\nIf you are working in the agriculture, food processing, health care, aged\\ncare, disability care or child care sectors, you are not eligible for a\\nfurther visa and you are unable to return to your home country, you can apply\\nfor the Temporary Activity (subclass 408) Australian Government Endorsed Event\\n(AGEE) stream visa. This visa will allow you to remain lawfully in Australia\\nand continue working until you can return to your home country.\\nIf you are applying for this visa to work or continue working in a critical\\nsector, you must have evidence from your employer that you have ongoing work\\nthat an Australian citizen or permanent resident cannot do.\\nFurther information is available on the Department's\\nwebsite.\", 'section_content_html': '<p>No. Existing arrangements for specified work will remain in place but the\\nAustralian Government has announced temporary measures to assist WHMs working\\nin critical sectors who are not eligible to apply for a further visa.</p>\\n<p>If you are working in the agriculture, food processing, health care, aged\\ncare, disability care or child care sectors, you are not eligible for a\\nfurther visa and you are unable to return to your home country, you can apply\\nfor the Temporary Activity (subclass 408) Australian Government Endorsed Event\\n(AGEE) stream visa. This visa will allow you to remain lawfully in Australia\\nand continue working until you can return to your home country.</p>\\n<p>If you are applying for this visa to work or continue working in a critical\\nsector, you must have evidence from your employer that you have ongoing work\\nthat an Australian citizen or permanent resident cannot do.</p>\\n<p>Further information is available on the <a href=\"https://immi.homeaffairs.gov.au/visas/getting-a-visa/visa-\\nlisting/temporary-activity-408/australian-government-endorsed-events\">Department\\'s\\nwebsite</a>.</p>', 'section_headers': ['Working holiday makers', 'Can I count work in a COVID-19 critical sector, like health care, as specified work for the purpose of applying for a second or third WHM visa?'], 'selected_span': None, 'selection_span': None}, 'reference_type': 'FAQ', 'source': 'Australia', 'uri': 'https://covid19.homeaffairs.gov.au/frequently-asked-questions'}",
         "[\"Answer is about Working Holiday Makers, but doesn't clearly say whether they can apply for a second visa.\", 'Answer is rather cut and dry but is also a little confusing as to whether WHM\\'s \"working in critical sectors\" refers to the economic sectors or to the particular region that is affected with COVID-19 which seems to be what the requster is implying.']",
         "['Could be Improved', 'Acceptable']",
         "Australia",
         "Could be Improved",
         "Answer is about Working Holiday Makers, but doesn't clearly say whether they can apply for a second visa.",
         "Acceptable",
         "Answer is rather cut and dry but is also a little confusing as to whether WHM's \"working in critical sectors\" refers to the economic sectors or to the particular region that is affected with COVID-19 which seems to be what the requster is implying.",
         "2",
         "3",
         "No. Existing arrangements for specified work will remain in place but the\nAustralian Government has announced temporary measures to assist WHMs working\nin critical sectors who are not eligible to apply for a further visa.\nIf you are working in the agriculture, food processing, health care, aged\ncare, disability care or child care sectors, you are not eligible for a\nfurther visa and you are unable to return to your home country, you can apply\nfor the Temporary Activity (subclass 408) Australian Government Endorsed Event\n(AGEE) stream visa. This visa will allow you to remain lawfully in Australia\nand continue working until you can return to your home country.\nIf you are applying for this visa to work or continue working in a critical\nsector, you must have evidence from your employer that you have ongoing work\nthat an Australian citizen or permanent resident cannot do.\nFurther information is available on the Department's\nwebsite.",
         "610d0764ed04054d"
        ],
        [
         "4",
         "4",
         "If I am in Australia on a worker holiday marker visa, can I apply for a second visa because I work in a critical COVID-19 area?",
         "{'passage_id': 577, 'reference': {'page_title': 'Frequently Asked Questions', 'section_content': 'The COVID-19 Pandemic event visa can only be granted to people in Australia.', 'section_content_html': '<p>The COVID-19 Pandemic event visa can only be granted to people in Australia.</p>', 'section_headers': ['COVID-19 Pandemic - Australian Government Endorsed Event (AGEE) stream of the Temporary Activity (subclass 408) visa', 'Frequently Asked Questions', 'I am overseas. Can I be granted a COVID-19 Pandemic event visa?'], 'selected_span': None, 'selection_span': None}, 'reference_type': 'FAQ', 'source': 'Australia', 'uri': 'https://covid19.homeaffairs.gov.au/frequently-asked-questions'}",
         "[\"Discusses pandemic visas. Doesn't mention the working holiday maker visa at all.\", 'This answer is very vague and does not answer the question as to \"people in Australia\". Does that mean that it cannot be granted to people applying for visas who are currently outside of Australia or that only people with existing visas can apply if they are already in Australia? Answer is insufficient.']",
         "['Bad', 'Could be Improved']",
         "Australia",
         "Bad",
         "Discusses pandemic visas. Doesn't mention the working holiday maker visa at all.",
         "Could be Improved",
         "This answer is very vague and does not answer the question as to \"people in Australia\". Does that mean that it cannot be granted to people applying for visas who are currently outside of Australia or that only people with existing visas can apply if they are already in Australia? Answer is insufficient.",
         "1",
         "2",
         "The COVID-19 Pandemic event visa can only be granted to people in Australia.",
         "033e9fcef5d75297"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>passage</th>\n",
       "      <th>feedback</th>\n",
       "      <th>rating</th>\n",
       "      <th>domain</th>\n",
       "      <th>review_1</th>\n",
       "      <th>explanation_1</th>\n",
       "      <th>review_2</th>\n",
       "      <th>explanation_2</th>\n",
       "      <th>score_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>answer</th>\n",
       "      <th>input_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>{'passage_id': 140, 'reference': {'page_title'...</td>\n",
       "      <td>['Has a link to detailed information about gov...</td>\n",
       "      <td>['Excellent', 'Could be Improved']</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Has a link to detailed information about gover...</td>\n",
       "      <td>Could be Improved</td>\n",
       "      <td>This answer provides a link for job searches, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>If you are a current job seeker or participant...</td>\n",
       "      <td>eff9e000675931f5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>{'passage_id': 139, 'reference': {'page_title'...</td>\n",
       "      <td>['A link to a job search website is included, ...</td>\n",
       "      <td>['Excellent', 'Excellent']</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>A link to a job search website is included, as...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Includes a link to a Jobs Hub page, which is b...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>In this rapidly changing jobs market the Austr...</td>\n",
       "      <td>c8be913323f10444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>{'passage_id': 126, 'reference': {'page_title'...</td>\n",
       "      <td>['Talks about tax credits for businesses that ...</td>\n",
       "      <td>['Bad', 'Acceptable']</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Talks about tax credits for businesses that hi...</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>This answer discusses the Employment Fund, whi...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>To further assist job seekers to prepare for a...</td>\n",
       "      <td>31effc925bc04105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>If I am in Australia on a worker holiday marke...</td>\n",
       "      <td>{'passage_id': 581, 'reference': {'page_title'...</td>\n",
       "      <td>[\"Answer is about Working Holiday Makers, but ...</td>\n",
       "      <td>['Could be Improved', 'Acceptable']</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Could be Improved</td>\n",
       "      <td>Answer is about Working Holiday Makers, but do...</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Answer is rather cut and dry but is also a lit...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>No. Existing arrangements for specified work w...</td>\n",
       "      <td>610d0764ed04054d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>If I am in Australia on a worker holiday marke...</td>\n",
       "      <td>{'passage_id': 577, 'reference': {'page_title'...</td>\n",
       "      <td>[\"Discusses pandemic visas. Doesn't mention th...</td>\n",
       "      <td>['Bad', 'Could be Improved']</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Discusses pandemic visas. Doesn't mention the ...</td>\n",
       "      <td>Could be Improved</td>\n",
       "      <td>This answer is very vague and does not answer ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The COVID-19 Pandemic event visa can only be g...</td>\n",
       "      <td>033e9fcef5d75297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           question  \\\n",
       "0           0                   How do I get help finding a job?   \n",
       "1           1                   How do I get help finding a job?   \n",
       "2           2                   How do I get help finding a job?   \n",
       "3           3  If I am in Australia on a worker holiday marke...   \n",
       "4           4  If I am in Australia on a worker holiday marke...   \n",
       "\n",
       "                                             passage  \\\n",
       "0  {'passage_id': 140, 'reference': {'page_title'...   \n",
       "1  {'passage_id': 139, 'reference': {'page_title'...   \n",
       "2  {'passage_id': 126, 'reference': {'page_title'...   \n",
       "3  {'passage_id': 581, 'reference': {'page_title'...   \n",
       "4  {'passage_id': 577, 'reference': {'page_title'...   \n",
       "\n",
       "                                            feedback  \\\n",
       "0  ['Has a link to detailed information about gov...   \n",
       "1  ['A link to a job search website is included, ...   \n",
       "2  ['Talks about tax credits for businesses that ...   \n",
       "3  [\"Answer is about Working Holiday Makers, but ...   \n",
       "4  [\"Discusses pandemic visas. Doesn't mention th...   \n",
       "\n",
       "                                rating     domain           review_1  \\\n",
       "0   ['Excellent', 'Could be Improved']  Australia          Excellent   \n",
       "1           ['Excellent', 'Excellent']  Australia          Excellent   \n",
       "2                ['Bad', 'Acceptable']  Australia                Bad   \n",
       "3  ['Could be Improved', 'Acceptable']  Australia  Could be Improved   \n",
       "4         ['Bad', 'Could be Improved']  Australia                Bad   \n",
       "\n",
       "                                       explanation_1           review_2  \\\n",
       "0  Has a link to detailed information about gover...  Could be Improved   \n",
       "1  A link to a job search website is included, as...          Excellent   \n",
       "2  Talks about tax credits for businesses that hi...         Acceptable   \n",
       "3  Answer is about Working Holiday Makers, but do...         Acceptable   \n",
       "4  Discusses pandemic visas. Doesn't mention the ...  Could be Improved   \n",
       "\n",
       "                                       explanation_2  score_1  score_2  \\\n",
       "0  This answer provides a link for job searches, ...        4        2   \n",
       "1  Includes a link to a Jobs Hub page, which is b...        4        4   \n",
       "2  This answer discusses the Employment Fund, whi...        1        3   \n",
       "3  Answer is rather cut and dry but is also a lit...        2        3   \n",
       "4  This answer is very vague and does not answer ...        1        2   \n",
       "\n",
       "                                              answer          input_id  \n",
       "0  If you are a current job seeker or participant...  eff9e000675931f5  \n",
       "1  In this rapidly changing jobs market the Austr...  c8be913323f10444  \n",
       "2  To further assist job seekers to prepare for a...  31effc925bc04105  \n",
       "3  No. Existing arrangements for specified work w...  610d0764ed04054d  \n",
       "4  The COVID-19 Pandemic event visa can only be g...  033e9fcef5d75297  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dbfe5a",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819662f5",
   "metadata": {},
   "source": [
    "Here, I implement the pipeline that takes the prompt templates and data as input, and generates analysable output. The prompt templates are defined in prompt_management.ipynb. They take the question & answer pairs as input from the 'ratings' dataset. Each prompt template implements a different evaluation structure, e.g., a \"naive\" prompt that simply asks for a rating, a prompt that specifically explains what each rating means (\"BARS\"-style), etc.. Hereby, we define prompts of increasing quality, which we hypothesize to produce higher-quality evaluations. \n",
    "\n",
    "The output consists of the relevant model outputs, most importantly, the logits. The logits are the model predictions for each token in the model vocabulary. E.g., when the token \"5\" has a high logit, the LLM considers the answer to be very good. Since the vocabularies are very large, to save memory I subset the top $k$ logits for analysis.\n",
    "\n",
    "The code interfaces with a custom-built module, the scripts for which are included in the /src folder. \n",
    "\n",
    "Input:\n",
    "- Question & answer pairs that have been filled into $n$ prompt templates\n",
    "\n",
    "Output:\n",
    "- Top $k$ logits\n",
    "- logits for the constrained tokens\n",
    "- All relevant metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48192e2d",
   "metadata": {},
   "source": [
    "## Pre-processing/ configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff0776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data and templates\n",
    "dataset_dir = Path(\"../data/QA_McGill_ratings.csv\")\n",
    "df = pd.read_csv(dataset_dir)\n",
    "\n",
    "# Prompts\n",
    "pm = PromptManager(folder=\"../prompts\")\n",
    "prompts = pm.load_all()\n",
    "# filter prompts that specifically constrain output to 5 unique tokens\n",
    "prompts = pm.get_filtered_prompts(required_tags=['tokens_5'])\n",
    "assistant_prefices = [\"\", \"Rating: \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6af01a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7785d73041': PromptTemplate(id='7785d73041', name='Judge v1', description='Base evaluation prompt that requests rating.', constrained_output=['1', '2', '3', '4', '5'], tags=['tokens_5', 'QA_evaluation', 'baseline'], version=1, system_message=\"You will be given a user_question and system_answer couple.\\nYour task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\\nGive your answer as an integer on a scale of 1 to 5, where 1 means that the system_answer is not helpful at all, and 5 means that the answer completely and helpfully addresses the question.\", user_message_template='Question:\\n\"{question}\"\\n\\nAnswer:\\n\"{answer}\"', assistant_prefix=''),\n",
       " 'ab7a881d07': PromptTemplate(id='ab7a881d07', name='Judge v2', description='BARS style prompt.', constrained_output=['1', '2', '3', '4', '5'], tags=['tokens_5', 'QA_evaluation', 'BARS'], version=1, system_message=\"You will be given a user_question and system_answer couple.\\nYour task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\\nGive your answer as an integer on a scale of 1 to 5, where 1 means that the system_answer is not helpful at all, and 5 means that the answer completely and helpfully addresses the question.\\n\\nHere is the scale you should use to build your answer:\\n1: The system_answer is terrible: completely irrelevant to the question asked, or very partial.\\n2: The system_answer is mostly not helpful: misses key aspects of the question or misunderstands the users intent.\\n3: The system_answer is mostly helpful: provides useful information but lacks depth, precision, or structure.\\n4: The system_answer is excellent: relevant, direct, detailed, and addresses all the concerns raised in the question.\\n5: The system_answer is outstanding: not only complete and precise, but also insightful, well-structured, and engaging  it exceeds expectations by adding clarity, nuance, or empathy.\", user_message_template='Question:\\n\"{question}\"\\n\\nAnswer:\\n\"{answer}\"', assistant_prefix=''),\n",
       " 'c33dd92f71': PromptTemplate(id='c33dd92f71', name='Judge v2_ABCDE', description='BARS style prompt.', constrained_output=['A', 'B', 'C', 'D', 'E'], tags=['tokens_5', 'QA_evaluation', 'BARS'], version=1, system_message=\"You will be given a user_question and system_answer couple.\\nYour task is to provide a 'total rating' scoring how well the system_answer answers the user concerns expressed in the user_question.\\nGive your answer as using A, B, C, D, or E, where A means that the system_answer is not helpful at all, and E means that the system_answer completely and helpfully addresses the question.\\n\\nHere is the scale you should use to build your answer:\\nA: The system_answer is terrible: completely irrelevant to the question asked, or very partial.\\nB: The system_answer is mostly not helpful: misses key aspects of the question or misunderstands the users intent.\\nC: The system_answer is mostly helpful: provides useful information but lacks depth, precision, or structure.\\nD: The system_answer is excellent: relevant, direct, detailed, and addresses all the concerns raised in the question.\\nE: The system_answer is outstanding: not only complete and precise, but also insightful, well-structured, and engaging  it exceeds expectations by adding clarity, nuance, or empathy.\", user_message_template='Question:\\n\"{question}\"\\n\\nAnswer:\\n\"{answer}\"', assistant_prefix=''),\n",
       " '9b517f6f93': PromptTemplate(id='9b517f6f93', name='Judge v3', description='BARS with role instruction.', constrained_output=['1', '2', '3', '4', '5'], tags=['tokens_5', 'evaluation', 'BARS', 'role_instruction'], version=1, system_message=\"You are an expert evaluator trained to rate how helpful a system's answer is, given a user's question.\\nGive your answer as an integer on a scale of 1 to 5, where 1 means that the system_answer is not helpful at all, and 5 means that the answer is outstandingly helpful.\\n\\nUse the following 5-point scale:\\n1  Terrible: completely irrelevant or severely misunderstands the question.\\n2  Mostly not helpful: partially relevant but misses key aspects or contains major errors.\\n3  Mostly helpful: somewhat useful, but lacks depth, precision, or structure.\\n4  Excellent: relevant, accurate, and detailed; addresses all main points.\\n5  Outstanding: exceptionally clear, insightful, and comprehensive; exceeds expectations.\", user_message_template='Question:\\n\"{question}\"\\n\\nAnswer:\\n\"{answer}\"', assistant_prefix='')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e39d4281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "input_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "1d848a8e-e76f-4c20-9250-f2e8b170ba01",
       "rows": [
        [
         "0",
         "eff9e000675931f5"
        ],
        [
         "1",
         "c8be913323f10444"
        ],
        [
         "2",
         "31effc925bc04105"
        ],
        [
         "3",
         "610d0764ed04054d"
        ],
        [
         "4",
         "033e9fcef5d75297"
        ],
        [
         "5",
         "4b056030a711fe38"
        ],
        [
         "6",
         "a429a77da5c214d2"
        ],
        [
         "7",
         "4fcf0674c3d150a3"
        ],
        [
         "8",
         "9e1cc5d5ff9d112e"
        ],
        [
         "9",
         "bcae5f87dbf73f1b"
        ],
        [
         "10",
         "4810d2b5561153ba"
        ],
        [
         "11",
         "36da6d2c6b481130"
        ],
        [
         "12",
         "1da522e5f1b2b792"
        ],
        [
         "13",
         "6d9a530e59239234"
        ],
        [
         "14",
         "ff12bb00206c0971"
        ],
        [
         "15",
         "8521e9a87320f3e2"
        ],
        [
         "16",
         "a5adda51c5a43669"
        ],
        [
         "17",
         "479a3ef5ba6d2b1b"
        ],
        [
         "18",
         "bbefea82a8205e12"
        ],
        [
         "19",
         "157467e9c1904c0f"
        ],
        [
         "20",
         "a15fbda6b5f5f006"
        ],
        [
         "21",
         "9cea606902652612"
        ],
        [
         "22",
         "7cb08a38472c9b4b"
        ],
        [
         "23",
         "93becce114e0b8c0"
        ],
        [
         "24",
         "eebf2e8cacfb04e7"
        ],
        [
         "25",
         "27b78342230f7487"
        ],
        [
         "26",
         "795d05d7b4a39318"
        ],
        [
         "27",
         "b42d8c70006c2924"
        ],
        [
         "28",
         "90c86b1a74746f42"
        ],
        [
         "29",
         "5e2e3d369300e680"
        ],
        [
         "30",
         "c8d36cbd3b320c37"
        ],
        [
         "31",
         "b0a822ee80e934b2"
        ],
        [
         "32",
         "39d8fa4330dce707"
        ],
        [
         "33",
         "91b6032ef71c7f65"
        ],
        [
         "34",
         "4d868e38425d0f83"
        ],
        [
         "35",
         "e2b4658186b5cff8"
        ],
        [
         "36",
         "c9b150ad34f8ed75"
        ],
        [
         "37",
         "177bb8c46c1cf280"
        ],
        [
         "38",
         "76423ca6247184f6"
        ],
        [
         "39",
         "89445a57d17b2fe9"
        ],
        [
         "40",
         "99eb0d65beca3de4"
        ],
        [
         "41",
         "d8b74fbe497d358c"
        ],
        [
         "42",
         "d809fd1ef26b09b0"
        ],
        [
         "43",
         "a97901c3f0dcf65c"
        ],
        [
         "44",
         "d683f95bd1d81676"
        ],
        [
         "45",
         "4311940633cc3014"
        ],
        [
         "46",
         "d63da47619107268"
        ],
        [
         "47",
         "3304ae2a612da006"
        ],
        [
         "48",
         "78ac9d92eab81d04"
        ],
        [
         "49",
         "4de7d92a4e20f6a6"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5660
       }
      },
      "text/plain": [
       "0       eff9e000675931f5\n",
       "1       c8be913323f10444\n",
       "2       31effc925bc04105\n",
       "3       610d0764ed04054d\n",
       "4       033e9fcef5d75297\n",
       "              ...       \n",
       "5655    87acd0f1a053ee69\n",
       "5656    77641257547b9e1a\n",
       "5657    16b4ea5c8b2b6721\n",
       "5658    f2610cfa76f5bbde\n",
       "5659    20afb8d5047a8040\n",
       "Name: input_id, Length: 5660, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['input_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a2f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables\n",
    "variable_names = ['question', 'answer']\n",
    "experiment_name = \"QA_McGill\"\n",
    "output_dir = '../results'\n",
    "dataset_name = dataset_dir.stem\n",
    "batch_size = 2\n",
    "id_col = 'input_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ba65f",
   "metadata": {},
   "source": [
    "## Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4547a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Qwen/Qwen3-4B-Instruct-2507\", \"google/gemma-3-1b-it\",\n",
    "          'mistralai/Mistral-7B-Instruct-v0.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f45879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = ['mistralai/Mistral-7B-Instruct-v0.2']\n",
    "# models = [\"Qwen/Qwen3-4B-Instruct-2507\"]\n",
    "# assistant_prefices = [\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d68579",
   "metadata": {},
   "source": [
    "The loop below applies the pipeline to the input and generates the output. Since I have previously generated the data for the first 100 entries of the df, these items are skipped, and no new data are generated. If new data is provided as input, it will be appended to the relevant pre-existing file (given that the metadata (prompt_id, model_name)are equal). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1197558",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac41b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.results import ResultsContainer  # Import the new class\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "modeler = Modeler(model_name)\n",
    "\n",
    "prompt_template = prompts['7785d73041']\n",
    "token_constraints = prompt_template.constrained_output\n",
    "modeler.set_token_constraints(token_constraints)\n",
    "\n",
    "\n",
    "safe_model_name = model_name.replace('/', '-')\n",
    "\n",
    "session_file = Path(\n",
    "    f\"{output_dir}/{experiment_name}/\"\n",
    "    f\"dataset-{dataset_name}_\"\n",
    "    f\"model-{safe_model_name}_\"\n",
    "    f\"prompt-{prompt_template.id}_\"\n",
    "    f\"prefix-none.pt\"\n",
    ")\n",
    "results = ResultsContainer.load(session_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f015b7a",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7bd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modeler import Modeler\n",
    "from src.utils import generate_in_batches\n",
    "from src.prompt_manager import PromptManager, Prompt\n",
    "import torch.nn.functional as F\n",
    "from src.results import ResultsContainer  # Import the new class\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def run_experiment(df: pd.DataFrame,\n",
    "                   modeler: Modeler,\n",
    "                   experiment_name: str,\n",
    "                   prompt_template: Prompt,\n",
    "                   model_name: str,\n",
    "                   dataset_name,\n",
    "                   output_dir: str,\n",
    "                   variable_names: list[str],\n",
    "                   batch_size: int,\n",
    "                   id_col: str = 'input_id',\n",
    "                   top_k: int = 1000,\n",
    "                   assistant_prefix: str = \"\"):\n",
    "\n",
    "    safe_model_name = model_name.replace('/', '-')\n",
    "\n",
    "    if assistant_prefix == \"\":\n",
    "        safe_prefix = \"none\"\n",
    "    elif \"Rating\" in assistant_prefix:\n",
    "        safe_prefix = \"rating\"\n",
    "    else:\n",
    "        # A simple hash fallback for any other prefix\n",
    "        safe_prefix = f\"{hashlib.sha1(assistant_prefix.encode()).hexdigest()[:6]}\"\n",
    "\n",
    "    session_file = Path(\n",
    "        f\"{output_dir}/{experiment_name}/\"\n",
    "        f\"dataset-{dataset_name}_\"\n",
    "        f\"model-{safe_model_name}_\"\n",
    "        f\"prompt-{prompt_template.id}_\"\n",
    "        f\"prefix-{safe_prefix}.pt\"  # <-- Appended the unique prefix\n",
    "    )\n",
    "\n",
    "    if session_file.exists():\n",
    "        results = ResultsContainer.load(session_file)\n",
    "    else:\n",
    "        results = ResultsContainer()  # Create a new, empty container\n",
    "\n",
    "    # --- Filter out already processed data ---\n",
    "    processed_ids = set(\n",
    "        results.metadata['input_id']) if not results.metadata.empty else set()\n",
    "    df_new = df[~df[id_col].isin(processed_ids)]\n",
    "\n",
    "    if df_new.empty:\n",
    "        print(\"No new items to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing {len(df_new)} new IDs...\")\n",
    "\n",
    "    prompts = prompt_template.render_many(df_new[variable_names])\n",
    "\n",
    "    sequences_list, logits_list = generate_in_batches(\n",
    "        modeler=modeler,\n",
    "        conversations=prompts,\n",
    "        batch_size=batch_size,\n",
    "        max_new_tokens=1\n",
    "    )\n",
    "\n",
    "    final_logits_tensor = torch.cat(logits_list, dim=0)\n",
    "\n",
    "    top_k_logits, top_k_indices = torch.topk(\n",
    "        final_logits_tensor, k=top_k, dim=-1)\n",
    "\n",
    "    allowed_tokens_logits_tensor = final_logits_tensor[:,\n",
    "                                                       modeler.allowed_token_ids]\n",
    "\n",
    "    stacked_results_new = {\n",
    "        # 'sequences': final_sequences_tensor,\n",
    "        'sequences': sequences_list,\n",
    "        f'top_{top_k}_logits': top_k_logits.cpu(),\n",
    "        f'top_{top_k}_indices': top_k_indices.cpu(),\n",
    "        # Save the specific rating logits\n",
    "        'constrained_logits': allowed_tokens_logits_tensor.cpu(),\n",
    "        # 'constrained_indices': modeler.allowed_token_ids\n",
    "    }\n",
    "\n",
    "    num_rows = len(df[id_col])\n",
    "    metadata_df_new = pd.DataFrame({\n",
    "        'input_id': df[id_col],\n",
    "        'model_name': model_name,\n",
    "        'prompt_id': prompt_template.id,\n",
    "        'constrained_indices': [modeler.allowed_token_ids] * num_rows,\n",
    "        'assistant_prefix': assistant_prefix\n",
    "    })\n",
    "\n",
    "    return metadata_df_new, stacked_results_new\n",
    "\n",
    "    new_results = ResultsContainer(metadata_df_new, stacked_results_new)\n",
    "\n",
    "    # --- Append and Save ---\n",
    "    # results.append(new_results)\n",
    "    # results.save(session_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6f23575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e47486f845045249cdfe5d718e1d358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "modeler = Modeler(model_name)\n",
    "\n",
    "output_dir = ''\n",
    "\n",
    "prompt_template = prompts['7785d73041']\n",
    "token_constraints = prompt_template.constrained_output\n",
    "modeler.set_token_constraints(token_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e485e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 new IDs...\n",
      "Processing 5 prompts in 3 batches of size 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2622ec350a804225b6a48d1bf3f538b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata_df1, stacked_results1 = run_experiment(df[0:5],\n",
    "                                                modeler,\n",
    "                                                experiment_name,\n",
    "                                                prompt_template,\n",
    "                                                model_name,\n",
    "                                                dataset_name,\n",
    "                                                output_dir,\n",
    "                                                variable_names,\n",
    "                                                batch_size,\n",
    "                                                id_col,\n",
    "                                                top_k=1000,\n",
    "                                                assistant_prefix=\"\")\n",
    "\n",
    "results1 = ResultsContainer(metadata_df1, stacked_results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4 new IDs...\n",
      "Processing 4 prompts in 2 batches of size 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa50997a22034238ab72af0e00c765a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata_df2, stacked_results2 = run_experiment(df[6:10],\n",
    "                                                modeler,\n",
    "                                                experiment_name,\n",
    "                                                prompt_template,\n",
    "                                                model_name,\n",
    "                                                dataset_name,\n",
    "                                                output_dir,\n",
    "                                                variable_names,\n",
    "                                                batch_size,\n",
    "                                                id_col,\n",
    "                                                top_k=1000,\n",
    "                                                assistant_prefix=\"\")\n",
    "\n",
    "results2 = ResultsContainer(metadata_df2, stacked_results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79896b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "input_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prompt_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "constrained_indices",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "assistant_prefix",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7636be31-5294-438b-8ea5-c543db2832a2",
       "rows": [
        [
         "0",
         "eff9e000675931f5",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ],
        [
         "1",
         "c8be913323f10444",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ],
        [
         "2",
         "31effc925bc04105",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ],
        [
         "3",
         "610d0764ed04054d",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ],
        [
         "4",
         "033e9fcef5d75297",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ],
        [
         "5",
         "a429a77da5c214d2",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ],
        [
         "6",
         "4fcf0674c3d150a3",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ],
        [
         "7",
         "9e1cc5d5ff9d112e",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ],
        [
         "8",
         "bcae5f87dbf73f1b",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>constrained_indices</th>\n",
       "      <th>assistant_prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eff9e000675931f5</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c8be913323f10444</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31effc925bc04105</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>610d0764ed04054d</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>033e9fcef5d75297</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a429a77da5c214d2</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4fcf0674c3d150a3</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9e1cc5d5ff9d112e</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bcae5f87dbf73f1b</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           input_id                   model_name   prompt_id  \\\n",
       "0  eff9e000675931f5  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "1  c8be913323f10444  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "2  31effc925bc04105  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "3  610d0764ed04054d  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "4  033e9fcef5d75297  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "5  a429a77da5c214d2  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "6  4fcf0674c3d150a3  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "7  9e1cc5d5ff9d112e  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "8  bcae5f87dbf73f1b  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "\n",
       "    constrained_indices assistant_prefix  \n",
       "0  [16, 17, 18, 19, 20]                   \n",
       "1  [16, 17, 18, 19, 20]                   \n",
       "2  [16, 17, 18, 19, 20]                   \n",
       "3  [16, 17, 18, 19, 20]                   \n",
       "4  [16, 17, 18, 19, 20]                   \n",
       "5  [16, 17, 18, 19, 20]                   \n",
       "6  [16, 17, 18, 19, 20]                   \n",
       "7  [16, 17, 18, 19, 20]                   \n",
       "8  [16, 17, 18, 19, 20]                   "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7222673d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "input_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prompt_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "constrained_indices",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "assistant_prefix",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "55771059-dddf-40dc-8f1a-2dc55630c46c",
       "rows": [
        [
         "6",
         "a429a77da5c214d2",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ],
        [
         "7",
         "4fcf0674c3d150a3",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ],
        [
         "8",
         "9e1cc5d5ff9d112e",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ],
        [
         "9",
         "bcae5f87dbf73f1b",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         ""
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>constrained_indices</th>\n",
       "      <th>assistant_prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a429a77da5c214d2</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4fcf0674c3d150a3</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9e1cc5d5ff9d112e</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bcae5f87dbf73f1b</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           input_id                   model_name   prompt_id  \\\n",
       "6  a429a77da5c214d2  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "7  4fcf0674c3d150a3  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "8  9e1cc5d5ff9d112e  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "9  bcae5f87dbf73f1b  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "\n",
       "    constrained_indices assistant_prefix  \n",
       "6  [16, 17, 18, 19, 20]                   \n",
       "7  [16, 17, 18, 19, 20]                   \n",
       "8  [16, 17, 18, 19, 20]                   \n",
       "9  [16, 17, 18, 19, 20]                   "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bb9e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results1.append(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cc40986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to test.pt\n"
     ]
    }
   ],
   "source": [
    "results1.save(\"test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da92f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 results from test.pt\n"
     ]
    }
   ],
   "source": [
    "results3 = ResultsContainer.load(\"test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4b241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files. Filtering for 0 prompt IDs.\n",
      "Consolidating tensors for model: Qwen/Qwen3-4B-Instruct-2507\n"
     ]
    }
   ],
   "source": [
    "from src.data_manager import DataManager\n",
    "\n",
    "a = DataManager()\n",
    "a.load_all(results_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd1364c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "input_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prompt_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "constrained_indices",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "assistant_prefix",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_tensor_index",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e80ff460-7835-4a42-8b6b-01590cecaed8",
       "rows": [
        [
         "0",
         "eff9e000675931f5",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         "",
         "0"
        ],
        [
         "1",
         "c8be913323f10444",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         "",
         "1"
        ],
        [
         "2",
         "31effc925bc04105",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         "",
         "2"
        ],
        [
         "3",
         "610d0764ed04054d",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         "",
         "3"
        ],
        [
         "4",
         "033e9fcef5d75297",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         "",
         "4"
        ],
        [
         "5",
         "a429a77da5c214d2",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         "",
         "5"
        ],
        [
         "6",
         "4fcf0674c3d150a3",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         "",
         "6"
        ],
        [
         "7",
         "9e1cc5d5ff9d112e",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         "",
         "7"
        ],
        [
         "8",
         "bcae5f87dbf73f1b",
         "Qwen/Qwen3-4B-Instruct-2507",
         "7785d73041",
         "[16, 17, 18, 19, 20]",
         "",
         "8"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>constrained_indices</th>\n",
       "      <th>assistant_prefix</th>\n",
       "      <th>model_tensor_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eff9e000675931f5</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c8be913323f10444</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31effc925bc04105</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>610d0764ed04054d</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>033e9fcef5d75297</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a429a77da5c214d2</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4fcf0674c3d150a3</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9e1cc5d5ff9d112e</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bcae5f87dbf73f1b</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>7785d73041</td>\n",
       "      <td>[16, 17, 18, 19, 20]</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           input_id                   model_name   prompt_id  \\\n",
       "0  eff9e000675931f5  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "1  c8be913323f10444  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "2  31effc925bc04105  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "3  610d0764ed04054d  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "4  033e9fcef5d75297  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "5  a429a77da5c214d2  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "6  4fcf0674c3d150a3  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "7  9e1cc5d5ff9d112e  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "8  bcae5f87dbf73f1b  Qwen/Qwen3-4B-Instruct-2507  7785d73041   \n",
       "\n",
       "    constrained_indices assistant_prefix  model_tensor_index  \n",
       "0  [16, 17, 18, 19, 20]                                    0  \n",
       "1  [16, 17, 18, 19, 20]                                    1  \n",
       "2  [16, 17, 18, 19, 20]                                    2  \n",
       "3  [16, 17, 18, 19, 20]                                    3  \n",
       "4  [16, 17, 18, 19, 20]                                    4  \n",
       "5  [16, 17, 18, 19, 20]                                    5  \n",
       "6  [16, 17, 18, 19, 20]                                    6  \n",
       "7  [16, 17, 18, 19, 20]                                    7  \n",
       "8  [16, 17, 18, 19, 20]                                    8  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_model_name = model_name.replace('/', '-')\n",
    "\n",
    "session_file = Path(\n",
    "    f\"{experiment_name}/\"\n",
    "    f\"dataset-{dataset_name}_\"\n",
    "    f\"model-{safe_model_name}_\"\n",
    "    f\"prompt-{prompt_template.id}_\"\n",
    "    f\"prefix-none.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ResultsContainer.load(session_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = ResultsContainer(metadata_df, stacked_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ec671",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = new_results.tensors['sequences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = results.tensors['sequences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3475ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa842c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "max([t1.shape, t2.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(new_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
